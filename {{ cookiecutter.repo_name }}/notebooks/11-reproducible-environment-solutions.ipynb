{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Reproducible Environments\n",
    "(Continued from `README.md`)\n",
    "\n",
    "## Overview\n",
    "\n",
    "* Requirements: The Bare Minimum \n",
    "\n",
    "* Using a Data Science Template: `cookiecutter`\n",
    "\n",
    "* Virtual Environments: `conda` and environment files\n",
    "* Revision Control: git and a git workflow\n",
    "   * Installing, Enabling, and using nbdime\n",
    "* The Data Science DAG\n",
    "   * make, Makefiles and data flow\n",
    "* Python Modules\n",
    "   * Creating an editable module\n",
    "* Testing: doctest, pytest, hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start out by checking that all the requirements are met from the previous exercises (started in `README.md`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Install the requirements\n",
    "\n",
    "* Anaconda\n",
    "* Cookiecutter\n",
    "* make\n",
    "* git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda --version   # or `$CONDA_EXE --version` in some environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cookiecutter --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Start your cookiecutter-based project\n",
    "\n",
    "Create a project called `bus_number_tutorial`:\n",
    "\n",
    "    Use conda as your virtualenv manager\n",
    "    Use python 3.6 or greater\n",
    "\n",
    "When complete, you should have a fully populated project directory, complete with customized README.md.\n",
    "\n",
    "We will be working in this project from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "\n",
    "<pre>\n",
    " $ <b>cookiecutter cookiecutter-easydata</b>\n",
    "\n",
    "project_name [project_name]: <b>bus_number_tutorial</b>\n",
    "repo_name [bus_number]: <b>↵</b>\n",
    "module_name [src]: <b>↵</b>\n",
    "author_name [Your name (or your organization/company/team)]: <b>Kjell Wooding</b>\n",
    "description [A short description of this project.]: <b>Reproducible Data Science</b>\n",
    "Select open_source_license:\n",
    "1 - MIT\n",
    "2 - BSD-2-Clause\n",
    "3 - Proprietary\n",
    "Choose from 1, 2, 3 [1]: <b>↵</b>\n",
    "s3_bucket [[OPTIONAL] your-bucket-for-syncing-data (do not include 's3://')]: <b>↵</b>\n",
    "aws_profile [default]: <b>↵</b>\n",
    "Select virtualenv:\n",
    "1 - conda\n",
    "2 - virtualenv\n",
    "Choose from 1, 2 [1]: <b>↵</b>\n",
    "Select python_interpreter:\n",
    "1 - python3\n",
    "2 - python\n",
    "Choose from 1, 2 [1]: <b>↵</b>\n",
    "\n",
    "\n",
    " $ <b>cd bus_number_tutorial</b>\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b:\n",
    "\n",
    "Explore the `README.md` from your new `bus_number_tutorial` project\n",
    "\n",
    "(Hint: You can use the `%load` magic, or `!cat` to look at it in your notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bus_number_tutorial\n",
    "==============================\n",
    "\n",
    "Reproducible data science tutorial\n",
    "\n",
    "GETTING STARTED\n",
    "---------------\n",
    "\n",
    "For complete instructions, visit: https://github.com/hackalog/bus_number/wiki/Getting-Started\n",
    "\n",
    "* Create and switch to the  virtual environment:\n",
    "```\n",
    "cd bus_number_tutorial\n",
    "make create_environment\n",
    "conda activate bus_number_tutorial\n",
    "make requirements\n",
    "```\n",
    "* Explore the notebooks in the `notebooks` directory\n",
    "\n",
    "Project Organization\n",
    "------------\n",
    "* `LICENSE`\n",
    "* `Makefile`\n",
    "    * top-level makefile. Type `make` for a list of valid commands\n",
    "* `README.md`\n",
    "    * this file\n",
    "* `data`\n",
    "    * Data directory. often symlinked to a filesystem with lots of space\n",
    "    * `data/raw`\n",
    "        * Raw (immutable) hash-verified downloads\n",
    "    * `data/interim`\n",
    "        * Extracted and interim data representations\n",
    "    * `data/processed`\n",
    "        * The final, canonical data sets for modeling.\n",
    "* `docs`\n",
    "    * A default Sphinx project; see sphinx-doc.org for details\n",
    "* `models`\n",
    "    * Trained and serialized models, model predictions, or model summaries\n",
    "    * `models/trained`\n",
    "        * Trained models\n",
    "    * `models/output`\n",
    "        * predictions and transformations from the trained models\n",
    "* `notebooks`\n",
    "    *  Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "    the creator's initials, and a short `-` delimited description,\n",
    "    e.g. `1.0-jqp-initial-data-exploration`.\n",
    "* `references`\n",
    "    * Data dictionaries, manuals, and all other explanatory materials.\n",
    "* `reports`\n",
    "    * Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "    * `reports/figures`\n",
    "        * Generated graphics and figures to be used in reporting\n",
    "    * `reports/tables`\n",
    "        * Generated data tables to be used in reporting\n",
    "    * `reports/summary`\n",
    "        * Generated summary information to be used in reporting\n",
    "* `requirements.txt`\n",
    "    * (if using pip+virtualenv) The requirements file for reproducing the\n",
    "    analysis environment, e.g. generated with `pip freeze > requirements.txt`\n",
    "* `environment.yml`\n",
    "    * (if using conda) The YAML file for reproducing the analysis environment\n",
    "* `setup.py`\n",
    "    * Turns contents of `src` into a\n",
    "    pip-installable python module  (`pip install -e .`) so it can be\n",
    "    imported in python code\n",
    "* `src`\n",
    "    * Source code for use in this project.\n",
    "    * `src/__init__.py`\n",
    "        * Makes src a Python module\n",
    "    * `src/data`\n",
    "        * Scripts to fetch or generate data. In particular:\n",
    "        * `src/data/make_dataset.py`\n",
    "            * Run with `python -m src.data.make_dataset fetch`\n",
    "            or  `python -m src.data.make_dataset process`\n",
    "    * `src/analysis`\n",
    "        * Scripts to turn datasets into output products\n",
    "    * `src/models`\n",
    "        * Scripts to train models and then use trained models to make predictions.\n",
    "        e.g. `predict_model.py`, `train_model.py`\n",
    "* `tox.ini`\n",
    "    * tox file with settings for running tox; see tox.testrun.org\n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "<p><small>This project was built using <a target=\"_blank\" href=\"https://github.com/hackalog/cookiecutter-easydata\">cookiecutter-easydata</a>, an experimental fork of [cookiecutter-data-science](https://github.com/drivendata/cookiecutter-data-science) aimed at making your data science workflow reproducible.</small></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Set up your virtual environment and install all dependencies\n",
    "\n",
    "Create and activate your `bus_number_tutorial` conda environment using the above make commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `active environment` should be `bus_number_tutorial`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should also be able to import from `src`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if importing src doesn't work, try `make requirements`\n",
    "import src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Pick up this tutorial in your new repo\n",
    "\n",
    "* Copy the notebooks from `bus_number` into your new `bus_number_tutorial` repo\n",
    "* Run jupyter notebook and open `notebooks/10-reproducible-environment.ipynb`\n",
    "\n",
    "If you're currently running this notebook and the checks from the previous exercises worked, then you're in business!\n",
    "\n",
    "Keep going from here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision Control: `git`\n",
    "\n",
    "How do we keep track of our changes? We use **git**.\n",
    "\n",
    "Before we do anything interesting, let's initialize a git repository (repo) here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Initialize a git repo for `bus_number_tutorial`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial Import\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# On branch master\r\n",
      "nothing to commit, working directory clean\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see: \n",
    "    \n",
    "    # On branch master\n",
    "    nothing to commit, working directory clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get back to using git again soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Add a dependency\n",
    "Modify the environment file so that `make requirements` installs some additional packages\n",
    "* install `scikit-learn` using conda\n",
    "* install `nbdime` using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that you now have scikit-learn  and nbdime installed\n",
    "# Don't forget that you need to run `make requirements` once you've change the `environment.yml` file\n",
    "import sklearn\n",
    "import nbdime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/environment.lock b/environment.lock\r\n",
      "index 481b76a..792bac7 100644\r\n",
      "--- a/environment.lock\r\n",
      "+++ b/environment.lock\r\n",
      "@@ -10,6 +10,7 @@ dependencies:\r\n",
      "   - attrs=18.2.0=py_0\r\n",
      "   - babel=2.6.0=py_1\r\n",
      "   - backcall=0.1.0=py_0\r\n",
      "+  - blas=1.1=openblas\r\n",
      "   - bleach=3.1.0=py_0\r\n",
      "   - ca-certificates=2018.11.29=ha4d7672_0\r\n",
      "   - cairo=1.16.0=ha4e643d_1000\r\n",
      "@@ -62,8 +63,8 @@ dependencies:\r\n",
      "   - libxml2=2.9.8=h143f9aa_1005\r\n",
      "   - markupsafe=1.1.0=py36h14c3975_1000\r\n",
      "   - mistune=0.8.4=py36h14c3975_1000\r\n",
      "-  - mkl_fft=1.0.10=py36h14c3975_1\r\n",
      "-  - mkl_random=1.0.2=py36h637b7d7_2\r\n",
      "+  - mkl_fft=1.0.10=py36_0\r\n",
      "+  - mkl_random=1.0.2=py36_0\r\n",
      "   - more-itertools=4.3.0=py36_1000\r\n",
      "   - nb_conda=2.2.1=py36_0\r\n",
      "   - nb_conda_kernels=2.2.0=py36_1000\r\n",
      "@@ -72,6 +73,8 @@ dependencies:\r\n",
      "   - nbval=0.9.1=py_0\r\n",
      "   - ncurses=6.1=hf484d3e_1002\r\n",
      "   - notebook=5.7.4=py36_1000\r\n",
      "+  - numpy=1.16.1=py36_blas_openblash1522bff_0\r\n",
      "+  - openblas=0.3.3=h9ac9557_1001\r\n",
      "   - openssl=1.0.2p=h14c3975_1002\r\n",
      "   - packaging=19.0=py_0\r\n",
      "   - pandas=0.24.1=py36hf484d3e_0\r\n",
      "@@ -107,6 +110,8 @@ dependencies:\r\n",
      "   - qtconsole=4.4.3=py_0\r\n",
      "   - readline=7.0=hf8c457e_1001\r\n",
      "   - requests=2.21.0=py36_1000\r\n",
      "+  - scikit-learn=0.20.2=py36_blas_openblashebff5e3_1400\r\n",
      "+  - scipy=1.2.0=py36_blas_openblash1522bff_1201\r\n",
      "   - send2trash=1.5.0=py_0\r\n",
      "   - setuptools=40.8.0=py36_0\r\n",
      "   - sip=4.18.1=py36hf484d3e_1000\r\n",
      "@@ -140,12 +145,16 @@ dependencies:\r\n",
      "   - xz=5.2.4=h14c3975_1001\r\n",
      "   - zeromq=4.2.5=hf484d3e_1006\r\n",
      "   - zlib=1.2.11=h14c3975_1004\r\n",
      "-  - blas=1.0=mkl\r\n",
      "   - intel-openmp=2019.1=144\r\n",
      "+  - libopenblas=0.3.3=h5a2b251_3\r\n",
      "   - mkl=2019.1=144\r\n",
      "-  - numpy=1.15.4=py36h7e9f1db_0\r\n",
      "-  - numpy-base=1.15.4=py36hde5b4d6_0\r\n",
      "+  - numpy-base=1.15.4=py36h2f8d375_0\r\n",
      "   - pip:\r\n",
      "+    - colorama==0.4.1\r\n",
      "+    - gitdb2==2.0.5\r\n",
      "+    - gitpython==2.1.11\r\n",
      "+    - nbdime==1.0.4\r\n",
      "     - python-dotenv==0.10.1\r\n",
      "+    - smmap2==2.0.5\r\n",
      "     - src==0.0.1\r\n",
      " prefix: /home/ava00088/anaconda3/envs/bus_number_tutorial\r\n",
      "diff --git a/environment.yml b/environment.yml\r\n",
      "index 0462b1f..84eaca3 100644\r\n",
      "--- a/environment.yml\r\n",
      "+++ b/environment.yml\r\n",
      "@@ -6,7 +6,9 @@ dependencies:\r\n",
      "   - pip:\r\n",
      "     - -e .\r\n",
      "     - python-dotenv>=0.5.1\r\n",
      "+    - nbdime\r\n",
      "   - setuptools\r\n",
      "+  - scikit-learn\r\n",
      "   - wheel\r\n",
      "   - sphinx\r\n",
      "   - click\r\n",
      "diff --git a/notebooks/11-reproducible-environment-solutions.ipynb b/notebooks/11-reproducible-environment-solutions.ipynb\r\n",
      "index 36d265e..f4e6369 100644\r\n",
      "--- a/notebooks/11-reproducible-environment-solutions.ipynb\r\n",
      "+++ b/notebooks/11-reproducible-environment-solutions.ipynb\r\n",
      "@@ -290,7 +290,7 @@\r\n",
      "   },\r\n",
      "   {\r\n",
      "    \"cell_type\": \"code\",\r\n",
      "-   \"execution_count\": null,\r\n",
      "+   \"execution_count\": 1,\r\n",
      "    \"metadata\": {},\r\n",
      "    \"outputs\": [],\r\n",
      "    \"source\": [\r\n",
      "@@ -343,9 +343,18 @@\r\n",
      "   },\r\n",
      "   {\r\n",
      "    \"cell_type\": \"code\",\r\n",
      "-   \"execution_count\": null,\r\n",
      "-   \"metadata\": {},\r\n",
      "-   \"outputs\": [],\r\n",
      "+   \"execution_count\": 3,\r\n",
      "+   \"metadata\": {},\r\n",
      "+   \"outputs\": [\r\n",
      "+    {\r\n",
      "+     \"name\": \"stdout\",\r\n",
      "+     \"output_type\": \"stream\",\r\n",
      "+     \"text\": [\r\n",
      "+      \"# On branch master\\r\\n\",\r\n",
      "+      \"nothing to commit, working directory clean\\r\\n\"\r\n",
      "+     ]\r\n",
      "+    }\r\n",
      "+   ],\r\n",
      "    \"source\": [\r\n",
      "     \"!git status\"\r\n",
      "    ]\r\n",
      "@@ -379,9 +388,21 @@\r\n",
      "   },\r\n",
      "   {\r\n",
      "    \"cell_type\": \"code\",\r\n",
      "-   \"execution_count\": null,\r\n",
      "-   \"metadata\": {},\r\n",
      "-   \"outputs\": [],\r\n",
      "+   \"execution_count\": 4,\r\n",
      "+   \"metadata\": {},\r\n",
      "+   \"outputs\": [\r\n",
      "+    {\r\n",
      "+     \"ename\": \"ModuleNotFoundError\",\r\n",
      "+     \"evalue\": \"No module named 'sklearn'\",\r\n",
      "+     \"output_type\": \"error\",\r\n",
      "+     \"traceback\": [\r\n",
      "+      \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\r\n",
      "+      \"\\u001b[0;31mModuleNotFoundError\\u001b[0m                       Traceback (most recent call last)\",\r\n",
      "+      \"\\u001b[0;32m<ipython-input-4-8425208ba198>\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[0;31m# Check that you now have scikit-learn  and nbdime installed\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m \\u001b[0;31m# Don't forget that you need to run `make requirements` once you've change the `environment.yml` file\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 3\\u001b[0;31m \\u001b[0;32mimport\\u001b[0m \\u001b[0msklearn\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      4\\u001b[0m \\u001b[0;32mimport\\u001b[0m \\u001b[0mnbdime\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\r\n",
      "+      \"\\u001b[0;31mModuleNotFoundError\\u001b[0m: No module named 'sklearn'\"\r\n",
      "+     ]\r\n",
      "+    }\r\n",
      "+   ],\r\n",
      "    \"source\": [\r\n",
      "     \"# Check that you now have scikit-learn  and nbdime installed\\n\",\r\n",
      "     \"# Don't forget that you need to run `make requirements` once you've change the `environment.yml` file\\n\",\r\n",
      "@@ -1229,7 +1250,7 @@\r\n",
      "    \"name\": \"python\",\r\n",
      "    \"nbconvert_exporter\": \"python\",\r\n",
      "    \"pygments_lexer\": \"ipython3\",\r\n",
      "-   \"version\": \"3.7.2\"\r\n",
      "+   \"version\": \"3.6.7\"\r\n",
      "   }\r\n",
      "  },\r\n",
      "  \"nbformat\": 4,\r\n"
     ]
    }
   ],
   "source": [
    "!git diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6\n",
    "Your `environment.yml` should look like this now:\n",
    "<pre>\n",
    "name: bus_number_tutorial\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - pip\n",
    "  - pip:\n",
    "    - -e .\n",
    "    - python-dotenv>=0.5.1\n",
    "    <b>- nbdime</b>\n",
    "  - setuptools\n",
    "  - wheel\n",
    "  - sphinx\n",
    "  - click\n",
    "  - coverage\n",
    "  - pytest-cov\n",
    "  - jupyter\n",
    "  <b>- joblib</b>\n",
    "  - nb_conda\n",
    "  - nbval\n",
    "  - pandas\n",
    "  - requests\n",
    "  - python>=3.6\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be able to see the difference via git\n",
    "!git diff ../environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Basic git interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what has changed with git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# On branch master\r\n",
      "# Changes not staged for commit:\r\n",
      "#   (use \"git add <file>...\" to update what will be committed)\r\n",
      "#   (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n",
      "#\r\n",
      "#\tmodified:   ../environment.lock\r\n",
      "#\tmodified:   ../environment.yml\r\n",
      "#\tmodified:   11-reproducible-environment-solutions.ipynb\r\n",
      "#\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/environment.yml b/environment.yml\r\n",
      "index 0462b1f..84eaca3 100644\r\n",
      "--- a/environment.yml\r\n",
      "+++ b/environment.yml\r\n",
      "@@ -6,7 +6,9 @@ dependencies:\r\n",
      "   - pip:\r\n",
      "     - -e .\r\n",
      "     - python-dotenv>=0.5.1\r\n",
      "+    - nbdime\r\n",
      "   - setuptools\r\n",
      "+  - scikit-learn\r\n",
      "   - wheel\r\n",
      "   - sphinx\r\n",
      "   - click\r\n"
     ]
    }
   ],
   "source": [
    "!git diff -u ../environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add or reject the changes incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git add -p\n",
    "#!git reset -p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commit the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git commit -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have no differences in your branch now\n",
    "# Except for those that you've made by running notebooks\n",
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science DAG\n",
    "DAG = Directed Acyclic Graph. \n",
    "\n",
    "That means the process eventually stops. (This is a good thing!) \n",
    "\n",
    "It also means we can use a super old, but incredibly handy tool to implement this workflow: `make`.\n",
    "\n",
    "### Make, Makefiles, and the Data Flow\n",
    "\n",
    "\n",
    "We use a `Makefile` to organize and invoke the various steps in our Data Science pipeline.\n",
    "You have already used this file when you created your virtual environment in the first place:\n",
    "```\n",
    "make create_environment\n",
    "```\n",
    "Here are the steps we will be working through in this tutorial:\n",
    "<img src=\"references/cheat_sheet.png\" alt=\"Reproducible Data Science Workflow\" width=\"400\"/>\n",
    "\n",
    "A [PDF version of the cheat sheet](references/cheat_sheet.pdf) is also available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What's my make target doing?\n",
    "If you are ever curious what commands a `make` command will invoke (including any invoked dependencies), use `make -n`, which lists the commands without executing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 test_environment.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd .. && make -n requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a cute **self-documenting makefiles trick** (borrowed from `cookiecutter-datascience`) to make it easy to document the various targets that you add. This documentation is produced when you type a plain `make`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get started:\n",
      "  >>> \u001b[1mmake create_environment\u001b[m\n",
      "  >>> \u001b[1mconda activate bus_number_tutorial\u001b[m\n",
      "\n",
      "\u001b[1mProject Variables:\u001b[m\n",
      "PROJECT_NAME = bus_number_tutorial\n",
      "\n",
      "\u001b[1mAvailable rules:\u001b[m\n",
      "\u001b[36manalysis           \u001b[m Convert predictions / transforms / experiments into output \n",
      "                    data \n",
      "\u001b[36mclean              \u001b[m Delete all compiled Python files \n",
      "\u001b[36mclean_interim      \u001b[m Delete all interim (DataSource) files \n",
      "\u001b[36mclean_models       \u001b[m Delete all trained models \n",
      "\u001b[36mclean_predictions  \u001b[m Delete all predictions \n",
      "\u001b[36mclean_processed    \u001b[m Delete all processed datasets \n",
      "\u001b[36mclean_raw          \u001b[m Delete the raw downloads directory \n",
      "\u001b[36mcreate_environment \u001b[m Set up python interpreter environment \n",
      "\u001b[36mdata               \u001b[m convert raw datasets into fully processed datasets \n",
      "\u001b[36mlint               \u001b[m Lint using flake8 \n",
      "\u001b[36mpredict            \u001b[m predict / transform / run experiments \n",
      "\u001b[36mrequirements       \u001b[m Install or update Python Dependencies \n",
      "\u001b[36msources            \u001b[m Fetch, Unpack, and Process raw DataSources \n",
      "\u001b[36msync_data_from_s3  \u001b[m Download Data from S3 \n",
      "\u001b[36msync_data_to_s3    \u001b[m Upload Data to S3 \n",
      "\u001b[36mtest               \u001b[m Run all Unit Tests \n",
      "\u001b[36mtest_environment   \u001b[m Test python environment is set-up correctly \n",
      "\u001b[36mtrain              \u001b[m train / fit / build models \n",
      "\u001b[36mtransform_data     \u001b[m Apply Transformations to produce fully processed Datsets \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd .. && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood: The Format of a Makefile\n",
    "\n",
    "```\n",
    "## Comment to appear in the auto-generated documentation\n",
    "thing_to_build: space separated list of dependencies\n",
    "\tcommand_to_run            # there is a tab before this command.\n",
    "\tanother_command_to_run    # every line gets run in a *new shell*\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: What does this makefile print when you run `make train`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Makefile.test\n",
    "\n",
    "data: raw\n",
    "\t@echo \"Build Datasets\"\n",
    "train_test_split:\n",
    "\t@echo \"do train/test split\"\n",
    "train: data transform_data train_test_split\n",
    "\t@echo \"Train Models\"\n",
    "transform_data:\n",
    "\t@echo \"do a data transformation\"\n",
    "raw:\n",
    "\t@echo \"Fetch raw data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you see: ```*** missing separator.  Stop.``` it's because you have used spaces instead of **tabs** before your commands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "make -f Makefile.test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: What happens when you add a cycle to a Makefile\n",
    "Set up a makefile with a cyclic dependency and run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Makefile.test\n",
    "\n",
    "cycle: cycle_b\n",
    "\t@echo \"in a Makefile\"\n",
    "cycle_b: cycle_c\n",
    "\t@echo \"have a cycle\"\n",
    "cycle_c: cycle\n",
    "\t@echo \"You can't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "make -f Makefile.test cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Makefile like this is an easy way to set up a process flow expressed as a Directed Acyclic Graph (DAG).\n",
    "\n",
    "**Note**: We have only scratched the surface here. The are lots of interesting tricks you can do with make.\n",
    "* http://zmjones.com/make/\n",
    "* http://blog.byronjsmith.com/makefile-shortcuts.html\n",
    "* https://www.gnu.org/software/make/manual/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Revision Control: git workflows\n",
    "\n",
    "Git isn't really a collaboration tool. It's more a tool for implementing collaboration workflows.\n",
    "\n",
    "What do we mean by workflow? A process built on top of git that incorporates **pull requests** and **branches**. Typically, this is provided by sites like: GitHub, GitLab, BitBucket.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some useful references if `gitflow` isn't second nature to you yet\n",
    "* Introduction to GitHub tutorial: https://lab.github.com/githubtraining/introduction-to-github\n",
    "* Git Handbook: https://guides.github.com/introduction/git-handbook/\n",
    "* GitHub workflow cheatsheet: https://github.com/hackalog/bus_number/wiki/Github-Workflow-Cheat-Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life Rules for using `git`\n",
    "\n",
    "* Always work on a branch: `git checkout -b my_branch_name`. Delete branches once they are merged.\n",
    "* **Never** push to master. Always **work on a branch** and do a pull request.\n",
    "* Seriously, don't do work on master if you are collaborating with **anyone**.\n",
    "* If you pushed it anywhere, or shared it with anyone, don't `git rebase`. In fact, if you're reading this, don't `git rebase`. Save that for when you are comfortable solving git merge nightmares on your own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: \n",
    "\n",
    "Create a GitHub/GitLab/BitBucket repo and sync your repo to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11:\n",
    "* Create a branch called `add_sklearn`\n",
    "* Add a scikit-learn dependency\n",
    "* Check in these changes using git to your local repo\n",
    "* Push the new branch to GitHub\n",
    "* Create a pull request to merge this branch into master\n",
    "* Merge your PR (delete the branch afterwards)\n",
    "* Sync your local repo with GitHub, including deleting the merged branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should now only have a branch called master\n",
    "!git branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Modules\n",
    "By default, we keep our source code in a module called `src`. (this can be overridden in the cookieccutter)\n",
    "\n",
    "This is enabled via one line in `environment.yml`:\n",
    "```\n",
    "- pip:\n",
    "  - -e .\n",
    "```\n",
    "\n",
    "This creates an **editable module**, and looks in the current directory for a file called `setup.py` to indicate the module name and location"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %load ../setup.py\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "setup(\n",
    "    name='src',\n",
    "    packages=find_packages(),\n",
    "    version='0.0.1',\n",
    "    description='Up Your Bus Number: A Primer for Reproducible Data Science',\n",
    "    author='Tutte Institute for Mathematics and Computing',\n",
    "    license='MIT',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets you easily use your code in notebooks and other scripts, and avoids any `sys.path.append` silliness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASIDE: Semantic Versioning\n",
    "\n",
    "Semantic versioning (or *semver*), refers to the convention of versioning with a triple:\n",
    "\n",
    "    MAJOR.MINOR.PATCH\n",
    "\n",
    "With the following convention: when releasing new versions, increment the:\n",
    "\n",
    "*    MAJOR version when you make **incompatible API changes**,\n",
    "*    MINOR version when you **add functionality** in a backwards-compatible manner, and\n",
    "*    PATCH version when you make backwards-compatible **bug fixes**.\n",
    "\n",
    "If you have no other plan, this is a great convention to follow.\n",
    "\n",
    "For an obscene amount of detail on this concept, see https://semver.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12:\n",
    "* add your favorite utility function to `src/utils`\n",
    "* increment the version number of the editable package\n",
    "* run `make requirements` (required if you added dependencies for your utility function)\n",
    "* import your utility function and run it from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file -a ../src/utils.py\n",
    "def read_space_delimited(filename, skiprows=None, class_labels=True, metadata=None):\n",
    "    \"\"\"Read an space-delimited file\n",
    "    \n",
    "    Data is space-delimited. Last column is the (string) label for the data\n",
    "\n",
    "    Note: we can't use automatic comment detection, as `#` characters are also\n",
    "    used as data labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skiprows: list-like, int or callable, optional\n",
    "        list of rows to skip when reading the file. See `pandas.read_csv`\n",
    "        entry on `skiprows` for more\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class (target) label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_csv(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_space_delimited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_space_delimited?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: doctest, pytest, coverage\n",
    "\n",
    "\n",
    "Python has built in testing frameworks via:\n",
    "* doctests:https://docs.python.org/3/library/doctest.html#module-doctest\n",
    "* unittest: https://docs.python.org/3/library/unittest.html\n",
    "\n",
    "Additionally, you'll want to make regular use of:\n",
    "* pytest: https://docs.pytest.org/en/latest/\n",
    "* pytest-cov: https://pypi.org/project/pytest-cov/\n",
    "* hypothesis: https://hypothesis.readthedocs.io/en/latest\n",
    "\n",
    "Cookiecutter (vanilla flavoured) comes witha setup for the `tox` testing framework built in.\n",
    "* https://tox.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12:\n",
    "\n",
    "Add a `make test` target to your makefile that:\n",
    "* runs doctests\n",
    "* runs pytest unit tests\n",
    "* (extra credit) Displays test coverage results\n",
    "    \n",
    "When you run `make test`, you will find tests that will fail in `src/test_example.py`. Fix them in the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 12:\n",
    "\n",
    "    test:\n",
    "        cd src && pytest --doctest-modules --verbose --cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make -n test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cd src && pytest --doctest-modules --verbose --cov\n",
    "    ============================= test session starts ==============================\n",
    "    platform linux -- Python 3.6.7, pytest-4.2.1, py-1.7.0, pluggy-0.8.1 -- /opt/software/anaconda3/envs/bus_number_tutorial/bin/python\n",
    "    cachedir: .pytest_cache\n",
    "    rootdir: ~/src/devel/bus_number_tutorial, inifile:\n",
    "    plugins: cov-2.6.1, nbval-0.9.1\n",
    "    collected 7 items                                                              \n",
    "\n",
    "    test_example.py::src.test_example.addition FAILED                        [ 14%]\n",
    "    test_example.py::TestExercises::test_addition FAILED                     [ 28%]\n",
    "    data/fetch.py::src.data.fetch.available_hashes PASSED                    [ 42%]\n",
    "    data/fetch.py::src.data.fetch.fetch_file PASSED                          [ 57%]\n",
    "    data/fetch.py::src.data.fetch.fetch_files PASSED                         [ 71%]\n",
    "    data/fetch.py::src.data.fetch.get_dataset_filename PASSED                [ 85%]\n",
    "    data/utils.py::src.data.utils.normalize_labels PASSED                    [100%]\n",
    "\n",
    "    =================================== FAILURES ===================================\n",
    "    _____________________ [doctest] src.test_example.addition ______________________\n",
    "    004 \n",
    "    005     I'm a failing doctest. Please fix me.\n",
    "    006     >>> addition(10, 12)\n",
    "    Expected:\n",
    "        20\n",
    "    Got:\n",
    "        -2\n",
    "\n",
    "    ~/src/devel/bus_number_tutorial/src/test_example.py:6: DocTestFailure\n",
    "    _________________________ TestExercises.test_addition __________________________\n",
    "\n",
    "    self = <src.test_example.TestExercises testMethod=test_addition>\n",
    "\n",
    "        def test_addition(self):\n",
    "            \"\"\"\n",
    "            I'm a failing unittest. Fix me.\n",
    "            \"\"\"\n",
    "    >       assert subtraction(5, 5) == 0\n",
    "    E       AssertionError: assert 10 == 0\n",
    "    E         -10\n",
    "    E         +0\n",
    "\n",
    "    test_example.py:22: AssertionError\n",
    "\n",
    "    ----------- coverage: platform linux, python 3.6.7-final-0 -----------\n",
    "    Name                         Stmts   Miss  Cover\n",
    "    ------------------------------------------------\n",
    "    __init__.py                      0      0   100%\n",
    "    analysis/__init__.py             0      0   100%\n",
    "    analysis/analysis.py           105     86    18%\n",
    "    analysis/run_analysis.py        23      9    61%\n",
    "    data/__init__.py                 4      0   100%\n",
    "    data/apply_transforms.py        27     12    56%\n",
    "    data/datasets.py               311    262    16%\n",
    "    data/fetch.py                  143    109    24%\n",
    "    data/localdata.py                1      0   100%\n",
    "    data/make_dataset.py            15      4    73%\n",
    "    data/transform_data.py          88     72    18%\n",
    "    data/transformers.py            42     29    31%\n",
    "    data/utils.py                   85     61    28%\n",
    "    features/__init__.py             0      0   100%\n",
    "    features/build_features.py       0      0   100%\n",
    "    logging.py                       7      0   100%\n",
    "    models/__init__.py               3      0   100%\n",
    "    models/algorithms.py             5      4    20%\n",
    "    models/model_list.py            74     60    19%\n",
    "    models/predict.py              100     80    20%\n",
    "    models/predict_model.py         22      9    59%\n",
    "    models/train.py                 54     39    28%\n",
    "    models/train_models.py          25     11    56%\n",
    "    paths.py                        17      0   100%\n",
    "    test_example.py                  8      0   100%\n",
    "    utils.py                        58     45    22%\n",
    "    visualization/__init__.py        0      0   100%\n",
    "    visualization/visualize.py       0      0   100%\n",
    "    workflow.py                      8      0   100%\n",
    "    ------------------------------------------------\n",
    "    TOTAL                         1225    892    27%\n",
    "\n",
    "    ====================== 2 failed, 5 passed in 1.69 seconds ======================\n",
    "    make: *** [test] Error 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** `make test` is normally functionality built into `cookiecutter-easydata`. We're building it from scratch here for the sake of practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13:\n",
    "Fix the failing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../src/test_example.py\n",
    "import unittest\n",
    "\n",
    "def addition(n1, n2):\n",
    "    \"\"\"\n",
    "    I'm addition\n",
    "    >>> addition(10, 10)\n",
    "    20\n",
    "    \"\"\"\n",
    "    return n1 + n2\n",
    "\n",
    "def subtraction(n1, n2):\n",
    "    \"\"\"\n",
    "    I'm subtraction.\n",
    "    \"\"\"\n",
    "    return n1 - n2\n",
    "\n",
    "class TestExercises(unittest.TestCase):\n",
    "    def test_subtraction(self):\n",
    "        \"\"\"\n",
    "        I'm a failing unittest. Fix me.\n",
    "        \"\"\"\n",
    "        assert subtraction(5, 5) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should pass all tests now!\n",
    "!cd .. && make test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14:\n",
    "* Check in all your changes to git\n",
    "* Merge them into your master branch via a PR in GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
